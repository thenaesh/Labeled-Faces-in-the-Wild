{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Homework #3 - Labeled Faces in the Wild"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matriculation Numbers: A0124772E, A0136070R, A0121299A\n",
    "\n",
    "Email Addresses: a0124772@u.nus.edu, e0005572@u.nus.edu, e0008742@u.nus.edu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boilerplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###################\n",
    "##### IMPORTS #####\n",
    "###################\n",
    "\n",
    "# Standard Library\n",
    "import math\n",
    "\n",
    "# Numpy\n",
    "import numpy as np\n",
    "\n",
    "# SciKit\n",
    "from sklearn.model_selection import KFold, GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Image Processing\n",
    "import dlib\n",
    "import cv2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#####################################\n",
    "##### MODEL SELECTION FRAMEWORK #####\n",
    "#####################################\n",
    "\n",
    "class LearningModel():\n",
    "    def __init__(self):\n",
    "        self.X              = None    # original points\n",
    "        self.y              = None    # original classifications\n",
    "        self.trained_models = None\n",
    "        self.best_model     = None\n",
    "        \n",
    "    \n",
    "    def supplyDataset(self, all_samples, all_labels):\n",
    "        '''\n",
    "            Ensures there are as many labels as samples.\n",
    "            \n",
    "            Sets the training samples X.\n",
    "            Sets the training labels y.\n",
    "        '''\n",
    "        assert all_samples.shape[0] == all_labels.shape[0]\n",
    "        \n",
    "        self.X = all_samples\n",
    "        self.y = all_labels\n",
    "    \n",
    "    def trainAll(self, cross_validation_param=10):\n",
    "        '''\n",
    "            For each split of training and test sets (using k-fold),\n",
    "                * train the model on the training set\n",
    "                * compute E_in\n",
    "                * compute E_out\n",
    "                * compute F1_in\n",
    "                * compute F1_out\n",
    "            Store a list of the (model, E_in, E_out, F1_in, F1_out) tuples.\n",
    "        '''\n",
    "        self.preprocess()\n",
    "        \n",
    "        kf = KFold(n_splits=cross_validation_param)\n",
    "        self.trained_models = []\n",
    "        \n",
    "        for train_idx, test_idx in kf.split(self.X):\n",
    "            trained_model = self.train(self.X[train_idx], self.y[train_idx])\n",
    "            \n",
    "            E_in   = self.getError(trained_model, self.X[train_idx], self.y[train_idx])\n",
    "            E_out  = self.getError(trained_model, self.X[test_idx], self.y[test_idx])\n",
    "            F1_in  = self.getF1(trained_model, self.X[train_idx], self.y[train_idx])\n",
    "            F1_out = self.getF1(trained_model, self.X[test_idx], self.y[test_idx])\n",
    "            \n",
    "            self.trained_models.append((trained_model, E_in, E_out, F1_in, F1_out))\n",
    "        \n",
    "        self.postprocess()\n",
    "    \n",
    "    def getAverageErrors(self):\n",
    "        '''\n",
    "            Returns a pair of the in-sample error and the average out-of-sample error.\n",
    "        '''\n",
    "        sum_E_in  = 0\n",
    "        sum_E_out = 0\n",
    "        \n",
    "        for model in self.trained_models:\n",
    "            E_in  = model[1]\n",
    "            E_out = model[2]\n",
    "            \n",
    "            sum_E_in                 += E_in\n",
    "            sum_E_out                += E_out\n",
    "        \n",
    "        average_E_in                 = sum_E_in                 / len(self.trained_models)\n",
    "        average_E_out                = sum_E_out                / len(self.trained_models)\n",
    "        \n",
    "        return (average_E_in, average_E_out)\n",
    "    \n",
    "    def getAverageF1(self):\n",
    "        '''\n",
    "            Returns the F1 scores of all the models on the data designated as output (i.e. F1_out values)\n",
    "        '''\n",
    "        f1s = []\n",
    "        \n",
    "        for model in self.trained_models:\n",
    "            f1s.append(model[4])\n",
    "        \n",
    "        return np.mean(f1s)\n",
    "    \n",
    "    def getError(self, classifier, points, classifications):\n",
    "        '''\n",
    "            Calculate the error of a model over a label given a sample dataset and labels for it.\n",
    "            0/1-loss is used as this is a classification problem.\n",
    "        '''\n",
    "\n",
    "        # use the model to predict the classifications of all points in the test set\n",
    "        predicted_classifications = classifier.predict(points)\n",
    "\n",
    "        # calculate the error using 0/1 loss\n",
    "        N = predicted_classifications.shape[0]\n",
    "        assert N == classifications.shape[0]\n",
    "        num_misclassifications = 0\n",
    "        for i in range(0, N):\n",
    "            if predicted_classifications[i] != classifications[i]:\n",
    "                num_misclassifications += 1\n",
    "\n",
    "        return num_misclassifications/N\n",
    "    \n",
    "    def getF1(self, classifier, points, classifications):\n",
    "        '''\n",
    "            Compute the F1 score for specified model.\n",
    "        '''\n",
    "        predicted_classifications = classifier.predict(points)\n",
    "        return f1_score(classifications, predicted_classifications, average='micro')\n",
    "    \n",
    "    def getBestModel(self):\n",
    "        '''\n",
    "            Returns the best model by F1_out score.\n",
    "        '''\n",
    "        best_model, best_score = None, 0\n",
    "        \n",
    "        for model in self.trained_models:\n",
    "            if model[4] > best_score:\n",
    "                best_model, best_score = model, model[4]\n",
    "        \n",
    "        return best_model\n",
    "    \n",
    "    def removeJunkModels(self):\n",
    "        '''\n",
    "            Wipes out self.trained_models, puts the best model back in.\n",
    "            Do this if we are sure we only want to keep the best models.\n",
    "            All predictions, etc, will only use a single model after this is done.\n",
    "        '''\n",
    "        self.trained_models = [self.getBestModel()]\n",
    "    \n",
    "    def predict(self, points):\n",
    "        '''\n",
    "            Get the modal prediction across all the models.\n",
    "        '''\n",
    "        num_points = points.shape[0]\n",
    "        predictions_by_model = [] # ith element is prediction of all points by model i\n",
    "        predictions_by_point = [] # ith element is modal prediction of point i by all models\n",
    "        \n",
    "        for model in self.trained_models:\n",
    "            predictions_by_model.append(model[0].predict(points))\n",
    "        \n",
    "        predictions_by_model = np.array(predictions_by_model)\n",
    "        \n",
    "        for i in range(0, num_points):\n",
    "            point_i_modal_prediction = np.argmax(np.bincount(predictions_by_model[:, i]))\n",
    "            predictions_by_point.append(point_i_modal_prediction)\n",
    "            \n",
    "        predictions_by_point = np.array(predictions_by_point)\n",
    "        \n",
    "        return predictions_by_point\n",
    "    \n",
    "    def preprocess(self):\n",
    "        pass\n",
    "    \n",
    "    def train(self, points, classifications):\n",
    "        pass\n",
    "    \n",
    "    def postprocess(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "class ScalingPCA_Classifier():\n",
    "    '''\n",
    "        Encapsulates models which use scaling followed by PCA.\n",
    "        Allows predictions using the model.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, classifier, scaler, pca):\n",
    "        self.classifier = classifier\n",
    "        self.scaler     = scaler\n",
    "        self.pca        = pca\n",
    "    \n",
    "    def predict(self, points):\n",
    "        return self.classifier.predict(self.pca.transform(self.scaler.transform(points)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "######################################\n",
    "##### IMAGE PROCESSING FUNCTIONS #####\n",
    "######################################\n",
    "\n",
    "def getAlignedImage(X):\n",
    "    '''\n",
    "        Computes an aligned image given an original image.\n",
    "        Adapted from\n",
    "    '''\n",
    "    \n",
    "    # get file from http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2\n",
    "    # unzip from bz2\n",
    "    predictor_model = \"shape_predictor_68_face_landmarks.dat\"\n",
    "    face_detector = dlib.get_frontal_face_detector()\n",
    "    face_pose_predictor = dlib.shape_predictor(predictor_model)\n",
    "\n",
    "    h, w = 50, 37\n",
    "\n",
    "    # from http://www.learnopencv.com/average-face-opencv-c-python-tutorial/\n",
    "\n",
    "    # Compute similarity transform given two sets of two points.\n",
    "    # OpenCV requires 3 pairs of corresponding points.\n",
    "    # We are faking the third one.\n",
    "\n",
    "    def similarityTransform(inPoints, outPoints) :\n",
    "        s60 = math.sin(60*math.pi/180);\n",
    "        c60 = math.cos(60*math.pi/180);  \n",
    "        inPts = np.copy(inPoints).tolist();\n",
    "        outPts = np.copy(outPoints).tolist();\n",
    "        xin = c60*(inPts[0][0] - inPts[1][0]) - s60*(inPts[0][1] - inPts[1][1]) + inPts[1][0];\n",
    "        yin = s60*(inPts[0][0] - inPts[1][0]) + c60*(inPts[0][1] - inPts[1][1]) + inPts[1][1];\n",
    "        inPts.append([np.int(xin), np.int(yin)]);\n",
    "        xout = c60*(outPts[0][0] - outPts[1][0]) - s60*(outPts[0][1] - outPts[1][1]) + outPts[1][0];\n",
    "        yout = s60*(outPts[0][0] - outPts[1][0]) + c60*(outPts[0][1] - outPts[1][1]) + outPts[1][1];\n",
    "        outPts.append([np.int(xout), np.int(yout)]);\n",
    "        tform = cv2.estimateRigidTransform(np.array([inPts]), np.array([outPts]), False);\n",
    "        return tform;\n",
    "\n",
    "    def get_points(flat_images, h=h, w=w):\n",
    "        points = []\n",
    "        for i in range(len(flat_images)):\n",
    "            image = np.uint8(flat_images[i].reshape((h,w)))\n",
    "            points.append([])\n",
    "            detected_faces = face_detector(image, 1)\n",
    "            for j, face_rect in enumerate(detected_faces):\n",
    "                pose_landmarks = face_pose_predictor(image, face_rect)\n",
    "                for k in range(68): \n",
    "                    point = dlib.full_object_detection.part(pose_landmarks, k)\n",
    "                    points[i].append((point.x, point.y))\n",
    "                break\n",
    "        return points\n",
    "\n",
    "    # adapted from http://www.learnopencv.com/average-face-opencv-c-python-tutorial/\n",
    "    # align eye corners at 0.3h, 0.15w/0.85w\n",
    "    def align_eyes(allPoints, images, h=h, w=w, n=68):\n",
    "        # Eye corners\n",
    "        eyecornerDst = [ (np.int(0.15 * w ), np.int(h / 3)), (np.int(0.85 * w ), np.int(h / 3)) ];\n",
    "        imagesNorm = []; \n",
    "        numImages = len(images)\n",
    "        numCount = 0\n",
    "        # Warp images and trasnform landmarks to output coordinate system,\n",
    "        # and find average of transformed landmarks.  \n",
    "        for i in range(numImages):\n",
    "            # dont change image if no face detected\n",
    "            if len(allPoints[i]) == 0:\n",
    "                imagesNorm.append(images[i])\n",
    "                continue;\n",
    "            # Corners of the eye in input image\n",
    "            eyecornerSrc  = [ allPoints[i][36], allPoints[i][45] ] ;\n",
    "            # Compute similarity transform\n",
    "            tform = similarityTransform(eyecornerSrc, eyecornerDst);\n",
    "            # Apply similarity transformation\n",
    "            img = cv2.warpAffine(images[i], tform, (w,h));\n",
    "            # Calculate location of average landmark points.\n",
    "            imagesNorm.append(img);\n",
    "        return imagesNorm\n",
    "\n",
    "    def to_image(flat_images):\n",
    "        return [img.reshape((h,w)) for img in flat_images]\n",
    "\n",
    "    def image_to_np_array(images, h=h, w=w):\n",
    "        array = np.zeros((len(images), h*w))\n",
    "        for k in range(len(images)):\n",
    "            for i in range(h):\n",
    "                for j in range(w):\n",
    "                    array[k][i*37+j] = images[k][i][j]\n",
    "        return array\n",
    "\n",
    "    X_points = get_points(X)\n",
    "    X_images_norm = align_eyes(X_points, to_image(X))\n",
    "    X_align = image_to_np_array(X_images_norm)\n",
    "    \n",
    "    return X_align\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.917525773196\n"
     ]
    }
   ],
   "source": [
    "class ScalingPCA_NeuralNet(LearningModel):\n",
    "    def predict(self, points):\n",
    "        return super().predict(getAlignedImage(points))\n",
    "    \n",
    "    def preprocess(self):\n",
    "        self.X = getAlignedImage(self.X)\n",
    "    \n",
    "    def train(self, points, classifications):\n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(points)\n",
    "        \n",
    "        points_scaled = scaler.transform(points)\n",
    "        \n",
    "        pca = PCA(n_components=150, svd_solver='randomized', whiten=True)\n",
    "        pca.fit(points_scaled)\n",
    "        \n",
    "        points_scaled_pca = pca.transform(points_scaled)\n",
    "        \n",
    "        param_grid = {'solver': ['adam','lbfgs'], 'alpha': [1e-1, 1e-2, 1e-3]}\n",
    "        clf = GridSearchCV(MLPClassifier(), param_grid, scoring='f1_micro')\n",
    "        \n",
    "        clf = clf.fit(points_scaled_pca, classifications)\n",
    "        return ScalingPCA_Classifier(clf, scaler, pca)\n",
    "\n",
    "\n",
    "X = np.load('X_train.npy')\n",
    "y = np.load('y_train.npy')\n",
    "\n",
    "neuralnet = ScalingPCA_NeuralNet()\n",
    "\n",
    "neuralnet.supplyDataset(X, y)\n",
    "neuralnet.trainAll()\n",
    "neuralnet.removeJunkModels()\n",
    "\n",
    "print(neuralnet.getAverageF1())\n",
    "\n",
    "\n",
    "def print_labels_to_file(filename, labels):\n",
    "    fo = open(filename,'w')\n",
    "    fo.write('ImageId,PredictedClass\\n')\n",
    "    for i in range(labels.shape[0]):\n",
    "        fo.write(str(i) + ',' + str(labels[i])+'\\n')\n",
    "    fo.close()\n",
    "\n",
    "test_set = np.load('X_test.npy')\n",
    "\n",
    "print_labels_to_file('transform_pca_nn.csv', neuralnet.predict(test_set))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class SimpleSVM(LearningModel):\n",
    "    def train(self, points, classifications):\n",
    "        param_grid = {'C': [10**p for p in range(-2, 3)],\n",
    "                      'kernel': ['linear', 'poly'],\n",
    "                      'gamma': [1e-1, 1e-2, 1e-3],\n",
    "                      'degree': list(range(2, 5))}\n",
    "        clf = GridSearchCV(SVC(), param_grid, scoring='f1_micro')\n",
    "        \n",
    "        clf = clf.fit(points, classifications)\n",
    "        return clf\n",
    "\n",
    "X = np.load('X_train.npy')\n",
    "y = np.load('y_train.npy')\n",
    "\n",
    "svm = SimpleSVM()\n",
    "\n",
    "svm.supplyDataset(X, y)\n",
    "svm.trainAll()\n",
    "svm.removeJunkModels()\n",
    "\n",
    "print(svm.getAverageF1())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statement of (Team-Level) Individual Work\n",
    "\n",
    "Please initial (between the square brackets) one of the following statements.\n",
    "\n",
    "[X] I, A0124772E and A0136070R and A0121299A, certify that we have followed the CS 3244 Machine Learning class guidelines for homework assignments.  In particular, we expressly vow that we have followed the Facebook rule in discussing with others (out of our team) in doing the assignment and did not take notes (digital or printed) from the discussions.  \n",
    "\n",
    "[ ] I, <*substitute your matric number here*>, did not follow the class rules regarding the homework assignment, because of the following reason:\n",
    "\n",
    "<*Please fill in*>\n",
    "\n",
    "I suggest that I should be graded as follows:\n",
    "\n",
    "<*Please fill in*>\n",
    "\n",
    "### References\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
